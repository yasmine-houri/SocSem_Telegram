{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network analysis alt-tech all years Pushshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install zstandard\n",
    "# !pip install ujson\n",
    "# !pip install stopwordsiso\n",
    "# !pip install matplotlib\n",
    "# !pip install nltk\n",
    "# !pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import ujson\n",
    "import io\n",
    "import os\n",
    "\n",
    "# SNA\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "# Domain study\n",
    "from collections import Counter\n",
    "# import urlexpander\n",
    "\n",
    "import random\n",
    "random.seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\167266\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\167266\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\167266\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Error solving\n",
    "np.float = float\n",
    "np.int = int   #module 'numpy' has no attribute 'int'\n",
    "np.object = object    #module 'numpy' has no attribute 'object'\n",
    "np.bool = bool    #module 'numpy' has no attribute 'bool'\n",
    "np. typeDict = np.sctypeDict\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "import string\n",
    "import stopwordsiso as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages\n",
    "\n",
    "alt_df = pd.read_excel(\"E:\\\\._PhD\\\\Publications\\\\SocSem_Telegram\\\\Code\\\\alt_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = list(alt_df['text'].unique())\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of edges: 3818\n"
     ]
    }
   ],
   "source": [
    "# make edges direction B -> A (message was forwarded from B to A)\n",
    "edges = list(zip(alt_df['fwd_from'], alt_df['to_id']))\n",
    "print(\"Total number of edges:\", len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of non-None edge: 3427\n"
     ]
    }
   ],
   "source": [
    "# Delete None values\n",
    "edges = [e for e in edges if None not in e]\n",
    "edges = [e for e in edges if not any(np.isnan(x) for x in e)]\n",
    "print(\"Total number of non-None edge:\", len(edges))\n",
    "\n",
    "# Set weights\n",
    "weights = dict(Counter(edges)) # Counter of number of existing forwards between each pair of channels\n",
    "e = [(k[0], k[1], v) for k, v in weights.items()]\n",
    "\n",
    "# # Normalize weights in e by total number of messages forwarded from each node\n",
    "# e = [(k[0], k[1], v/len([x for x in edges if x[0] == k[0]])) for k, v in weights.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes: 356\n",
      "Number of duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "nodes = list(set([item for sublist in edges for item in sublist]))\n",
    "print(\"Total number of nodes:\", len(nodes))\n",
    "\n",
    "# check no duplicates\n",
    "print(\"Number of duplicates:\", len([k for k, v in dict(Counter(nodes)).items() if v > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of isolated nodes: 0\n"
     ]
    }
   ],
   "source": [
    "G=nx.DiGraph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_weighted_edges_from(e)\n",
    "print(\"Number of isolated nodes:\", len(list(nx.isolates(G))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 356 nodes and 607 edges\n"
     ]
    }
   ],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1116499975.0, 1446718427, {'weight': 18}), (1398738967.0, 1271719213, {'weight': 5}), (1398738967.0, 1161666782, {'weight': 5}), (1398738967.0, 1362651760, {'weight': 5}), (1398738967.0, 1201072738, {'weight': 5}), (1398738967.0, 1277771372, {'weight': 29}), (1398738967.0, 1250324144, {'weight': 5}), (1398738967.0, 1392836102, {'weight': 6}), (1398738967.0, 1314300626, {'weight': 4}), (1398738967.0, 1200042196, {'weight': 4})]\n"
     ]
    }
   ],
   "source": [
    "# Print head of list of edges with associated weights\n",
    "print(list(G.edges(data=True))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic community detetion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/taynaud/python-louvain.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1490308239: 45,\n",
       " 1101645400.0: 45,\n",
       " 1443613812.0: 44,\n",
       " 1421391763: 44,\n",
       " 1000787017.0: 43,\n",
       " 1007269511.0: 43,\n",
       " 1000611791: 43,\n",
       " 1476086765: 42,\n",
       " 1470334558.0: 42,\n",
       " 1001403366.0: 41}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detect Louvain communities\n",
    "\n",
    "from community import community_louvain # https://github.com/taynaud/python-louvain\n",
    "partition = community_louvain.best_partition(G.to_undirected()) # {node: community ID}\n",
    "p_descending = {k: v for k, v in sorted(partition.items(), key=lambda item: item[1], reverse=True)}\n",
    "dict(islice(p_descending.items(), 0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modularity of the partition: 0.714214331652633\n"
     ]
    }
   ],
   "source": [
    "modularity = community_louvain.modularity(partition, G.to_undirected())\n",
    "print(f\"Modularity of the partition: {modularity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict {C: L} where C is the community label and L is the list of nodes (channels) belonging to that community\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "communities = defaultdict(list)\n",
    "\n",
    "for key, value in sorted(partition.items()):\n",
    "    communities[value].append(key)\n",
    "\n",
    "# dict(islice(communities.items(), 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum community size: 2\n",
      "Maximum community size: 45\n",
      "Average community size: 7.739130434782608\n",
      "Share of communities of size 2: 1.0869565217391304\n"
     ]
    }
   ],
   "source": [
    "# Community size\n",
    "\n",
    "c_size = {k: len(v) for k,v in communities.items()}\n",
    "# dict(islice(c_size.items(), 0, 10))\n",
    "\n",
    "print(\"Minimum community size:\", min(c_size.values()))\n",
    "print(\"Maximum community size:\", max(c_size.values()))\n",
    "print(\"Average community size:\", sum(c_size.values())/len(c_size))\n",
    "print(\"Share of communities of size 2:\", sum([v for k,v in c_size.items() if v == 2])/len(c_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove communities of size inferior or equal to n\n",
    "nodes_to_remove = []\n",
    "for community_id, nodes_in_community in communities.items():\n",
    "  if len(nodes_in_community) <= 2:\n",
    "    nodes_to_remove.extend(nodes_in_community)\n",
    "\n",
    "partition2 = {node: community_id for node, community_id in partition.items() if node not in nodes_to_remove}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum community2 size: 3\n",
      "Maximum community2 size: 45\n",
      "Average community2 size: 14.571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Create communities from partition2\n",
    "\n",
    "communities2 = defaultdict(list)\n",
    "\n",
    "for key, value in sorted(partition2.items()):\n",
    "    communities2[value].append(key)\n",
    "\n",
    "# Create dictionary of community sizes from communities2\n",
    "c_size2 = {k: len(v) for k,v in communities2.items()}\n",
    "\n",
    "print(\"Minimum community2 size:\", min(c_size2.values()))\n",
    "print(\"Maximum community2 size:\", max(c_size2.values()))\n",
    "print(\"Average community2 size:\", sum(c_size2.values())/len(c_size2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save partition as pickle\n",
    "\n",
    "# with open('E:\\._PhD\\Publications\\SocSem_Telegram\\data\\partition.pkl', 'wb') as f:\n",
    "#     pickle.dump(partition, f)\n",
    "\n",
    "# with open('partition2.pkl', 'wb') as f:\n",
    "#     pickle.dump(partition2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save graph\n",
    "\n",
    "# Add the community label to nodes\n",
    "\n",
    "for node, community_id in partition.items():\n",
    "    G.nodes[node]['community'] = community_id\n",
    "\n",
    "nx.write_gexf(G, \"E:\\._PhD\\Publications\\SocSem_Telegram\\data\\graph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'community': 4}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes[1314300626]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
