{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Integrating LLM-based topic modelling and user community detection on Telegram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install zstandard\n",
    "# !pip install json\n",
    "# !pip install ujson\n",
    "# !pip install networkx\n",
    "# !pip install matplotlib\n",
    "# !pip install nltk\n",
    "# !pip install stopwordsiso\n",
    "# !pip install emoji\n",
    "# !pip install bs4\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import ujson\n",
    "import io\n",
    "import os\n",
    "\n",
    "# SNA\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "# Domain study\n",
    "from collections import Counter\n",
    "# import urlexpander\n",
    "\n",
    "import random\n",
    "random.seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\167266\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\167266\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Error solving\n",
    "np.float = float    \n",
    "np.int = int   #module 'numpy' has no attribute 'int'\n",
    "np.object = object    #module 'numpy' has no attribute 'object'\n",
    "np.bool = bool    #module 'numpy' has no attribute 'bool'\n",
    "np. typeDict = np.sctypeDict\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "import string\n",
    "import stopwordsiso as stopwords\n",
    "\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Zreader class because Zreader module will not load (https://github.com/pushshift/zreader)\n",
    "\n",
    "class Zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        '''Init method'''\n",
    "        self.fh = open(file,'rb')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstd.ZstdDecompressor()\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "\n",
    "    def readlines(self):\n",
    "        '''Generator method that creates an iterator for each line of JSON'''\n",
    "        while True:\n",
    "            chunk = self.reader.read(self.chunk_size).decode('utf-8', 'ignore')\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels\n",
    "\n",
    "file = \"E:/._PhD/Data/Telegram/PushshiftChannels/files-DO NOT DELETE/channels.ndjson.zst\"\n",
    "# Adjust chunk_size as necessary -- defaults to 16,384 if not specified\n",
    "reader = Zreader(file)\n",
    "\n",
    "# Read each line from the reader\n",
    "channels = []\n",
    "i=0\n",
    "for line in reader.readlines():\n",
    "    obj = json.loads(line)\n",
    "    channels.append(obj)\n",
    "#     print(obj)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of alt-tech platforms from Wikipedia (https://en.wikipedia.org/wiki/Alt-tech#List_of_alt-tech_platforms)\n",
    "# # Excluding mainstream social media platforms and inactive platforms we couldn't trace a URL for\n",
    "alt = {\"Gab\": \"gab.com\",\n",
    "\"Gettr\" : \"gettr.com\",\n",
    "\"Parler\" : \"parler.com\",\n",
    "\"Truth Social\" : \"truthsocial.com\",\n",
    "\"BitChute\" : \"bitchute.com\",\n",
    "\"DLive\" : \"dlive.tv\",\n",
    "\"DTube\" : \"d.tube\",\n",
    "\"Odysee\" : \"lbry.com\",\n",
    "\"PewTube\" : \"pewtube.com\",\n",
    "\"Rumble\" : \"rumble.com\",\n",
    "\"Triller\" : \"triller.co\",\n",
    "\"GiveSendGo\" : \"givesendgo.com\",\n",
    "\"Hatreon\" : \"hatreon.net\",\n",
    "\"SubscribeStar\" : \"subscribestar.com\",\n",
    "\"GoyFundMe\" : \"goyfundme.com\",\n",
    "\"MeWe\" : \"mewe.com\",\n",
    "\"Minds\" : \"minds.com\",\n",
    "\"Thinkspot\" : \"thinkspot.com\",\n",
    "\"Patriots.win\" : \"patriots.win\",\n",
    "\"Infogalactic\" : \"infogalactic.com\",\n",
    "\"Metapedia\" : \"metapedia.org\",\n",
    "\"8kun\" : \"8kun.top\",\n",
    "\"WASP Love\" : \"wasplove.com\",\n",
    "\"JustPaste.it\" : \"justpaste.it\",\n",
    "\"Epik\" : \"epik.com\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract domain from URL string\n",
    "\n",
    "def extract_domain(url, full=False):\n",
    "  # Define a regular expression pattern for extracting the domain\n",
    "  pattern = r\"(https?://|http?://|http.?)?(www\\d?\\.)?(?P<domain>[\\w\\.-]+\\.\\w+)(/\\S*)?\"\n",
    "  # Use re.match to search for the pattern at the beginning of the URL\n",
    "  match = re.match(pattern, url)\n",
    "  # Check if a match is found\n",
    "  if match:\n",
    "  # Extract the domain from the named group \"domain\"\n",
    "    if not full:\n",
    "        domain = match.group(\"domain\")\n",
    "        return domain\n",
    "    else:\n",
    "        return match\n",
    "  else:\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Create edge list\n",
    "\n",
    "# Define features to keep\n",
    "keep = ['date','to_id', 'fwd_from', 'id', 'message'] # only keep messages that contain URLs\n",
    "\n",
    "edges = []\n",
    "messages = []\n",
    "\n",
    "runner = 0\n",
    "\n",
    "with open('E:\\\\._PhD\\\\Data\\\\Telegram\\\\PushshiftChannels\\\\files-DO NOT DELETE\\\\messages.ndjson.zst', 'rb') as dec:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    reader = dctx.stream_reader(dec)\n",
    "    tr = io.TextIOWrapper(reader, encoding=\"utf-8\")\n",
    "    for line in tr:\n",
    "        line_str = line.encode(\"utf-8\")\n",
    "        msg = ujson.loads(line_str)\n",
    "        msg_f = {key: msg[key] for key in keep if (key in msg\n",
    "                                                and 'fwd_from' in msg.keys()\n",
    "                                                and msg['fwd_from'] != None\n",
    "                                                and '2018-01' in msg['date']\n",
    "                                                )}\n",
    "        if msg_f != {}:\n",
    "            edges.append((msg_f['to_id']['channel_id'],msg_f['fwd_from']['channel_id']))\n",
    "            messages.append(msg_f)\n",
    "        if len(edges)%100 == 0 and len(edges) != runner:\n",
    "            runner = len(edges)\n",
    "            print(runner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1442"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3525"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\._PhD\\\\Data\\\\Telegram\\\\PushshiftChannels\\\\extractions\\\\altech_all_edges_extract_domain.json', 'wb') as f:\n",
    "    pickle.dump(edges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\._PhD\\\\Data\\\\Telegram\\\\PushshiftChannels\\\\extractions\\\\altech_all_messages_extract_domain.json', 'wb') as f:\n",
    "    pickle.dump(messages, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  672\n"
     ]
    }
   ],
   "source": [
    "nodes = [item for sublist in edges for item in sublist]\n",
    "\n",
    "# nodes.index(None)\n",
    "# # There are 2 empty values due to messages where no channel id is indicated in 'fwd_from'\n",
    "# # Delete\n",
    "nodes = [n for n in nodes if n != None]\n",
    "\n",
    "nodes = list(set(nodes))\n",
    "print(\"Number of nodes: \", len(nodes))\n",
    "\n",
    "# # check no duplicates\n",
    "# dup = {k: v for k, v in dict(Counter(nodes)).items() if v > 1}\n",
    "# len(dup)\n",
    "# del(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCOMING URLS BY CHANNELS\n",
    "\n",
    "in_urls = {k: [] for k in nodes} # keys are all channels\n",
    "for mm in messages:\n",
    "    urls = re.findall(r'https\\S+|http\\S+', mm['message'])\n",
    "    channel_id = mm['to_id']['channel_id']\n",
    "    in_urls[channel_id].append(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTGOING URLS BY CHANNELS\n",
    "\n",
    "out_urls = {k: [] for k in nodes} # keys are all channels\n",
    "for mm in messages:\n",
    "    urls = re.findall(r'https\\S+|http\\S+', mm['message'])\n",
    "    try: \n",
    "        channel_id = mm['fwd_from']['channel_id']\n",
    "        out_urls[channel_id].append(urls)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs shared:  12976\n",
      "Number of unique URLs:  2304\n"
     ]
    }
   ],
   "source": [
    "# Create the flat list of all shared urls\n",
    "all_urls = [x for v in in_urls.values() for x in v]\n",
    "all_urls.append([x for v in out_urls.values() for x in v])\n",
    "all_urls = [x for xs in all_urls for x in xs]\n",
    "a = [x for x in all_urls if type(x) != list]\n",
    "b = [u for u in all_urls if type(u) == list]\n",
    "b = [x for xs in b for x in xs]\n",
    "all_urls = a+b\n",
    "print(\"Total number of URLs shared: \", len(all_urls))\n",
    "print(\"Number of unique URLs: \", len(set(all_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012330456226880395"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in all_urls if \"bit.ly\" in x or \"tinyurl\" in x])/len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain from non-short links\n",
    "\n",
    "domains = [extract_domain(x) for x in all_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most shared domains\n",
    "\n",
    "counter_domains = dict(Counter(domains))\n",
    "# d_descending = {k: v for k, v in sorted(counter_domains.items(), key=lambda item: item[1], reverse=True)}\n",
    "# dict(islice(d_descending.items(), 0, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d.tube': 71,\n",
       " 'minds.com': 204,\n",
       " 'dlive.tv': 209,\n",
       " 'pewtube.com': 2,\n",
       " 'gab.com': 290,\n",
       " 'epik.com': 6,\n",
       " 'parler.com': 145,\n",
       " 'hatreon.net': 2,\n",
       " 'lbry.com': 8,\n",
       " 'justpaste.it': 731,\n",
       " 'subscribestar.com': 162,\n",
       " 'bitchute.com': 5922,\n",
       " 'thinkspot.com': 1,\n",
       " 'mewe.com': 162}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_alt = dict((k, counter_domains[k]) for k in list(set(alt.values()) & set(counter_domains.keys())))\n",
    "counter_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
